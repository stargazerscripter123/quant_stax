# LLM Quantâ€‘Tool ðŸª„

A unified PyTorch toolkit for Post-Training Quantization (PTQ) of Large Language Models with multiple backend support.

## Supported Quantization Methods

| `method`  | Backend                | Calib data? | Description |
|-----------|------------------------|-------------|-------------|
| `bnb_fp4` | **bitsandbytes FP4**   | No          | 4-bit FP4 quantization using bitsandbytes |
| `fp8`     | SmoothQuantâ€‘FP8        | Yes         | FP8 E4M3 quantization with SmoothQuant |
| `awq`     | LLM Compressor AWQ     | Yes         | Activation-aware Weight Quantization (INT4) |
| `gptq`    | **GPTQModel FP4**      | Yes         | GPTQ 4-bit quantization with GPU acceleration |

## Installation

### Quick Start
```bash
# Install base dependencies
pip install -e .

# Install specific quantization backends
pip install -e .[bnb_fp4]     # bitsandbytes FP4
pip install -e .[fp8]         # SmoothQuant FP8 (Linux only)
pip install -e .[awq]         # LLM Compressor AWQ
pip install -e .[gptq]        # GPTQModel GPTQ

# Install all backends
pip install -e .[bnb_fp4,fp8,awq,gptq]
```

### Using Conda Environment
```bash
# Create and activate environment
conda env create -f environment.yml
conda activate llm-quant-tool

# The environment.yml includes all optional dependencies
```

## Usage Examples

### BitsAndBytes FP4 (No calibration data needed)
```bash
python -m llm_quant_tool.cli -c configs/bnb_fp4.yaml
```

### SmoothQuant FP8 (Requires calibration)
```bash
python -m llm_quant_tool.cli -c configs/fp8.yaml
```

### AWQ INT4 (Requires calibration)
```bash
python -m llm_quant_tool.cli -c configs/awq.yaml
```

### GPTQ INT4 (Requires calibration, GPU accelerated)
```bash
python -m llm_quant_tool.cli -c configs/gptq.yaml
```

## Configuration

Each quantization method has its own configuration file in the `configs/` directory:
- `configs/bnb_fp4.yaml` - bitsandbytes FP4 configuration
- `configs/fp8.yaml` - SmoothQuant FP8 configuration  
- `configs/awq.yaml` - AWQ configuration
- `configs/gptq.yaml` - GPTQ configuration

## Requirements

- Python 3.9+
- PyTorch 2.2.0+
- CUDA 11.8+ (for GPU acceleration)
- 8GB+ GPU memory (recommended for larger models)

## Project Structure

```
llm_quant_tool/
â”œâ”€â”€ cli.py              # Command-line interface
â”œâ”€â”€ config.py           # Configuration handling
â”œâ”€â”€ data.py             # Dataset preparation
â”œâ”€â”€ evaluate.py         # Model evaluation utilities
â””â”€â”€ quant/              # Quantization backends
    â”œâ”€â”€ __init__.py     # Quantization dispatcher
    â”œâ”€â”€ awq.py          # AWQ quantization
    â”œâ”€â”€ bnb_fp4.py      # BitsAndBytes FP4
    â”œâ”€â”€ fp8.py          # SmoothQuant FP8
    â””â”€â”€ gptq.py         # GPTQ quantization

configs/                # Configuration files
â”œâ”€â”€ awq.yaml
â”œâ”€â”€ bnb_fp4.yaml
â”œâ”€â”€ fp8.yaml
â””â”€â”€ gptq.yaml
```

### Running Tests
```bash
pip install -e .[dev]
pytest test/
```
